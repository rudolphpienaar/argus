# fedml.manifest.yaml
#
# Persona manifest for the Federated ML (SeaGaP-MP) workflow.
# Defines the complete conversational DAG from dataset discovery
# through federated model publication.
#
# 19 stages across 4 phases. Every stage produces an artifact.

authors: ATLAS Team <atlas@childrens.harvard.edu>
name: "Federated ML — SeaGaP-MP Pipeline"
description: >
  Full lifecycle from dataset discovery through federated model publication.
  Search, Gather, Harmonize, Code, Train, Federate, Execute, Publish.
category: Federated Learning
persona: fedml
version: 1.2.0
locked: false

stages:

  # ─── Phase 1: Search & Gather ────────────────────────────────

  - id: search
    name: Dataset Discovery
    phase: search-and-gather
    previous: ~
    optional: true
    produces:
      - search.json
    parameters:
      keywords: null
      modality: null
      anatomy: null
      catalog: atlas
    narrative: Scanning ATLAS catalog indices and ranking candidate cohorts.
    blueprint:
      - Querying ATLAS catalog shards for cohort candidates.
      - Scoring modality/provider relevance and confidence.
      - Materializing search snapshot artifact in ~/searches/.
    instruction: >
      Search the ATLAS catalog for datasets matching your research criteria.
      Use keywords, modality filters, or anatomical region. Each search
      appends to the discovery log — you can search as many times as needed.
      Add datasets to your selection buffer with 'add <id>'.
    commands:
      - search <keywords>
      - add <dataset>
      - remove <dataset>
      - show cohort
    handler: search
    skip_warning:
      short: Dataset discovery not performed.
      reason: >
        Federated learning requires a clearly defined cohort of datasets.
        The search stage records the criteria used to identify your data,
        which is essential for scientific reproducibility and provenance.
      max_warnings: 2

  - id: gather
    name: Cohort Assembly
    phase: search-and-gather
    previous: search
    optional: false
    produces:
      - gather.json
    parameters:
      auto_select: false
    narrative: Mounting cohort assets and validating provenance metadata.
    blueprint:
      - Resolving dataset identity and provenance signature.
      - Mounting cohort tree into project input workspace.
      - Writing gather receipts and session provenance artifacts.
    instruction: >
      When your selection is complete, run 'gather' to assemble the cohort and
      create your project workspace. This will materialize the data into
      the provenance tree.
    commands:
      - gather
      - mount
    handler: gather
    skip_warning:
      short: Cohort not assembled.
      reason: >
        A project cannot be created without an assembled cohort. The gather
        stage materializes the cohort into your virtual workspace and
        prepares the underlying filesystem for training.
      max_warnings: 2

  - id: ml-readiness
    name: ML Experiment Readiness
    phase: search-and-gather
    previous: gather
    optional: true
    produces:
      - ml-readiness.json
    parameters:
      objective: auto
      require_supervision: true
    narrative: Validating task coherence and supervision coverage before harmonization.
    blueprint:
      - Inspecting gathered cohorts for task type and annotation shape.
      - Measuring supervision coverage and objective compatibility.
      - Materializing readiness report with pass/fail rationale.
    instruction: >
      Optionally validate that your gathered cohort is a meaningful ML
      experiment before continuing. This stage detects mixed-task
      incompatibilities (classification vs detection vs segmentation)
      and missing supervision artifacts.
    commands:
      - ml-readiness
      - readiness
    handler: ml-readiness

  - id: collect
    name: Cohort Collection Build
    phase: search-and-gather
    previous: ml-readiness
    optional: true
    produces:
      - collect.json
    parameters:
      split_seed: 42
      train_ratio: 0.8
      validation_ratio: 0.2
    narrative: Reorganizing gathered cohorts into a normalized collection layout.
    blueprint:
      - Classifying gathered cohorts by supervision task.
      - Building a normalized collection tree for downstream harmonization.
      - Emitting collection manifest and split receipts.
    instruction: >
      Optionally build a normalized collection view after readiness checks.
      This stage does not alter gathered evidence; it creates a structured
      reorganization suitable for downstream harmonization and model prep.
    commands:
      - collect
    handler: collect

  - id: join_gather_collect
    name: Causal Convergence (Gather/Readiness/Collect)
    phase: search-and-gather
    previous: [collect, ml-readiness]
    structural: true
    produces:
      - join.json
    handler: topological-join

  - id: pre_harmonize
    name: Harmonization View Resolver
    phase: harmonize
    previous: [join_gather_collect]
    structural: true
    produces:
      - resolve.json
    handler: pre-harmonize

  # ─── Phase 2: Harmonize ──────────────────────────────────────

  - id: harmonize
    name: Data Harmonization
    phase: harmonize
    previous: [pre_harmonize]
    optional: false
    produces:
      - harmonize.json
    parameters:
      resolution: [1.0, 1.0, 1.0]
      naming_convention: snake_case
      label_schema: default
    narrative: Standardizing site heterogeneity for federated readiness.
    blueprint:
      - Profiling cross-site metadata variance and label schema drift.
      - Applying normalization/resampling/alignment transforms.
      - Writing harmonization report and session state receipt.
    instruction: >
      Harmonize your cohort to ensure consistent data formats across sites.
      This applies image resampling, filename normalization, label schema
      alignment, and metadata standardization. Harmonization is critical
      for federated training — heterogeneous data causes silent failures.
    commands:
      - harmonize
    handler: harmonize
    skip_warning:
      short: Cohort not harmonized.
      reason: >
        Federated learning requires consistent data formats across sites.
        Without harmonization, your model may fail on heterogeneous inputs
        (different resolutions, naming conventions, or label schemas).
      max_warnings: 2

  - id: pre_code
    name: Workspace Continuity (Pre-Code)
    phase: code-and-validate
    previous: [harmonize]
    structural: true
    produces:
      - resolve.json
    handler: workspace-commit

  # ─── Phase 3: Code & Validate ────────────────────────────────

  - id: code
    name: Code Development
    phase: code-and-validate
    previous: pre_code
    optional: false
    produces:
      - code.json
    parameters:
      scaffold: true
      framework: flower
    narrative: Generating scaffold source and manifest assets.
    blueprint:
      - Injecting template parameters into training scaffold.
      - Materializing src/train.py and project Dockerfile.
      - Initializing local validation environment.
    instruction: >
      Develop your training code. The scaffold generates src/train.py with
      Flower client hooks, a Dockerfile, and plugin metadata. You can edit
      the code freely — each code/test cycle is recorded. When you're
      satisfied, move to local validation.
    commands:
      - proceed
      - code
    handler: scaffold
    skip_warning:
      short: Project structure not scaffolded.
      reason: >
        The FedML workflow requires a train.py with Flower client hooks and
        a properly configured Dockerfile for containerization. Scaffolding
        generates the required structure.
      max_warnings: 2

  - id: pre_train
    name: Workspace Continuity (Pre-Train)
    phase: code-and-validate
    previous: [code]
    structural: true
    produces:
      - resolve.json
    handler: workspace-commit

  - id: train
    name: Local Validation
    phase: code-and-validate
    previous: pre_train
    optional: false
    produces:
      - train.json
    parameters:
      epochs: 1
      batch_size: 32
    narrative: Running local execution pass and collecting validation artifacts.
    blueprint:
      - Executing local Phantom training loop simulation.
      - Validating data loader and model architecture compatibility.
      - Writing local passing receipt and validation metrics.
    instruction: >
      Run your training code locally to validate it before federation.
      Federation distributes your code to remote sites where debugging is
      difficult or impossible. A successful local run creates a .local_pass
      marker. You can iterate — go back to 'code' if tests fail.
    commands:
      - python train.py
      - train
    handler: train
    skip_warning:
      short: Local training not validated.
      reason: >
        Federation distributes your code to remote sites where debugging
        is difficult or impossible. Running locally first catches import
        errors, data loading issues, model architecture problems, and
        resource constraints.
      max_warnings: 2

  # ─── Phase 4: Federation ─────────────────────────────────────

  - id: federate-brief
    name: Federation Briefing
    phase: federation
    previous: train
    optional: false
    produces:
      - briefing.json
    parameters: {}
    narrative: Preparing federation orchestration and execution artifacts.
    blueprint:
      - Resolving active project context and participant site list.
      - Initializing federation stage pipeline and project publish config.
      - Displaying multi-phase execution strategy summary.
    instruction: >
      Review the federation briefing. This summarizes what will happen:
      your code will be transcompiled for Flower, containerized as a
      MERIDIAN-compliant OCI image, published to the ChRIS store, and
      dispatched to the federation network. Confirm to proceed.
    commands:
      - federate
    handler: federate-brief

  - id: federate-transcompile
    name: Flower Transcompilation
    phase: federation
    previous: federate-brief
    optional: false
    produces:
      - transcompile.json
    parameters:
      flower_version: "1.5"
      strategy: fedavg
    narrative: Flower transcompilation suite executing.
    blueprint:
      - Parsing training loop and data loader contracts.
      - Injecting Flower client/server hooks and adapters.
      - Emitting federated entrypoint and execution hooks.
    instruction: >
      Your training code is being transcompiled for the Flower federated
      learning framework. This wraps train.py in a Flower client, generates
      node.py for site-local execution, and adds the federation protocol
      hooks. Review the generated code if needed.
    commands:
      - show transcompile
      - transcompile
    handler: federate-transcompile

  - id: federate-containerize
    name: Container Build
    phase: federation
    previous: federate-transcompile
    optional: false
    produces:
      - containerize.json
    parameters:
      base_image: "python:3.11-slim"
      gpu: false
    narrative: Reproducible container image build in progress.
    blueprint:
      - Resolving base image and runtime dependencies.
      - Staging federated entrypoint and Flower hooks.
      - Building OCI image layers and writing SBOM.
    instruction: >
      Your transcompiled code is being built into a MERIDIAN-compliant
      OCI container image. This packages your code, dependencies, and
      Flower client into a reproducible container that can execute at
      any federation site.
    commands:
      - show container
      - containerize
    handler: federate-containerize

  - id: federate-publish-config
    name: Publication Configuration
    phase: federation
    previous: federate-containerize
    optional: false
    produces:
      - publish-config.json
    parameters:
      app_name: null
      organization: null
      visibility: private
    narrative: Finalizing publication metadata and visibility.
    blueprint:
      - Updating application identity and organization namespace.
      - Configuring registry access controls and visibility.
      - Confirming publication manifest for ChRIS store push.
    instruction: >
      Configure your application for publication to the ChRIS store.
      Set the application name, organization, and visibility. Private
      applications are only visible to your organization; public
      applications are discoverable by all ATLAS users.
    commands:
      - config name <app-name>
      - config org <organization>
      - config visibility <public|private>
      - publish-config
    handler: federate-publish-config

  - id: federate-publish-execute
    name: Registry Publication
    phase: federation
    previous: federate-publish-config
    optional: false
    produces:
      - publish-execute.json
    parameters: {}
    narrative: Pushing container image to global registry.
    blueprint:
      - Signing image reference and registry manifest.
      - Writing application metadata and publish receipts.
      - Validating global availability for federation dispatch.
    instruction: >
      Your container image is being pushed to the ChRIS store registry.
      This makes it available for federation dispatch. The publication
      record includes the image digest, registry URL, and metadata.
    commands:
      - show publish
      - publish-execute
    handler: federate-publish-execute

  - id: federate-dispatch
    name: Federation Dispatch
    phase: federation
    previous: federate-publish-execute
    optional: false
    produces:
      - dispatch.json
    parameters:
      sites: null
      rounds: 5
      timeout_minutes: 60
    narrative: Distributing container to trusted site domains.
    blueprint:
      - Resolving participant endpoints and secure handshakes.
      - Pushing OCI image to site-local compute clusters.
      - Triggering federated training job across the network.
      - "[ACTION: training_launch]"
    instruction: >
      Your application is being dispatched to the federation network.
      Select target sites or accept the default (all sites with matching
      data). The dispatch creates a federation job and begins coordination
      with the aggregator.
    commands:
      - dispatch
      - dispatch --sites <site1,site2>
    handler: federation-simulator

  - id: federate-execute
    name: Federated Training Execution
    phase: federation
    previous: federate-dispatch
    optional: false
    produces:
      - execute.json
    parameters:
      monitor_interval_s: 30
    narrative: Aggregator coordinating federated compute rounds.
    blueprint:
      - Monitoring real-time site participation and loss curves.
      - Capturing per-round weight updates and aggregation logs.
      - Materializing global model metrics and execution receipts.
    instruction: >
      Federated training is in progress. The aggregator coordinates
      training rounds across participating sites. You can monitor
      progress, view per-round metrics, and wait for completion.
      The execution record captures all rounds, site participation,
      and aggregated metrics.
    commands:
      - status
      - show metrics
      - show rounds
    handler: federate-execute

  - id: federate-model-publish
    name: Model Publication
    phase: federation
    previous: federate-execute
    optional: false
    produces:
      - model-publish.json
    parameters:
      model_name: null
      model_version: null
      description: null
    narrative: Attaching provenance chain and publishing to marketplace.
    blueprint:
      - Packaging aggregated model weights and metadata.
      - Linking Merkle provenance from search through execution.
      - Materializing marketplace catalog entry and public receipts.
    instruction: >
      Federated training is complete. The aggregated model weights are
      available in the aggregator's output directory. Publish the trained
      model to the ATLAS marketplace, making it discoverable by clinical
      users and other researchers. The publication record includes full
      provenance — from initial dataset search through every federation
      round.
    commands:
      - publish model
      - show provenance
    handler: federate-model-publish
