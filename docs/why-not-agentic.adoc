= Why ARGUS Doesn't Let the AI Drive: The Case for Deterministic Orchestration
:author: ATLAS Project Team
:revdate: 2026-02-17
:revnumber: 10.0.0
:toc: macro
:toclevels: 3
:sectnums:

toc::[]

== Abstract

This document presents the architectural rationale for ARGUS's rejection of the "autonomous agent" model in favor of a deterministic, human-in-the-loop framework. It outlines the structural risks of ungrounded agentic orchestration—specifically the Irreducible Hallucination problem—and details how ARGUS utilizes **Intent-First Design** to ensure safety, auditability, and scientific integrity in federated medical imaging workflows.

== Introduction: The Historical Context

The prevailing trend in modern artificial intelligence is toward "Agency"—the ability for models to autonomously plan, execute, and iterate through complex tasks. While this paradigm is compelling for general-purpose assistants, it is fundamentally brittle for medical research and clinical operations. Early experiments with "Autonomous Calypso" revealed that unconstrained agents inevitably suffer from "Instructional Decay." In these legacy versions, the AI's internal model of the project would eventually diverge from the physical reality of the data, leading the agent to "hallucinate" progress or skip critical validation steps (e.g., training a model on unharmonized data).

We learned that in high-stakes environments, the human operator must remain the authoritative decision-maker. The resolution was the implementation of **Deterministic Orchestration**. ARGUS uses the AI as a mediator and a compiler, but never as an autonomous controller. The system is designed so that the AI interprets what the user wants, but the underlying "Operating System" kernel (the Host) enforces the rules of the scientific process.

== The Structural Fragility of Probabilistic Loops

The fundamental flaw in autonomous agent systems is their probabilistic nature. Because LLMs operate by predicting the next token, their "reasoning" is a sequence of probabilities. When an agent is placed in an open-ended loop, these probabilities multiply.

*   **The Math of Failure:** If an agent has a 95% success rate for a single step, the reliability of a 10-step autonomous workflow drops to ~60% (0.95^10). For scientific research, this cumulative error rate is unacceptable.
*   **The Truth Gap:** Without a deterministic environment like the VCS, an agent has no way to verify its own claims. It can "believe" a file was created because it successfully generated the command, even if the command failed silently in the environment.

ARGUS solves this by grounding every probability in a deterministic state. The AI may *suggest* a project rename, but the renaming only occurs if the `rename` plugin executes successfully and materializes a physical artifact in the VFS.

== Intent-First Design: The IAS Separation

To manage the transition from human language to system mutation, ARGUS implements the **Intent-Action-State (IAS)** separation. This principle, drawn from the ChRIS lineage, ensures that the AI's role is strictly limited to the "Intent" phase.

1.  **Intents (Interpretation):** The user speaks in natural language. Calypso's `IntentParser` compiles this into a structured protocol intent.
2.  **Actions (Execution):** The Host dispatches the intent to a deterministic plugin. The plugin—not the AI—performs the work.
3.  **States (Materialization):** The result is recorded as a physical VFS artifact.

By isolating the AI from the "Action" and "State" phases, ARGUS ensures that the "Intelligence" layer can be replaced or updated without affecting the underlying procedural truth of the system.

== The Soft Enforcement Model: The Educational Gatekeeper

Rather than rigidly blocking users, ARGUS uses a "Soft Enforcement" model to guide them through the SeaGaP-MP workflow. When a user attempts to skip a stage (e.g., moving to code without gathering data), the system issues an educational warning.

*   **Warning 1:** A brief alert indicating the missing prerequisite.
*   **Warning 2:** A detailed explanation of why the stage is scientifically necessary.
*   **Override:** The user is eventually allowed to proceed, but only after acknowledging the risk.

This approach acknowledges that the human researcher is the expert. The system functions as a "Navigator," providing the map and the evidence, but leaving the steering to the human.

== Conclusion

ARGUS is designed to augment human intelligence, not replace it. By using a deterministic core and an Interpretation-First pipeline, we provide the efficiency of an agentic interface with the safety and auditable truth of a traditional operating system. The system's refusal to "let the AI drive" is its most important safety feature, ensuring that every federated model is the result of a verifiable, human-led scientific process.

---
_Last updated: 2026-02-17 (v10.0.0)_
