= ARGUS Safety Foundations: The Math of Agentic Integrity
:author: ATLAS Project Team
:revdate: 2026-02-17
:revnumber: 10.0.0
:toc: macro
:toclevels: 3
:sectnums:

toc::[]

== Abstract

This document provides the formal mathematical and architectural foundations for the ARGUS safety model. It analyzes the "Irreducible Hallucination" problem inherent in probabilistic AI systems and provides a proof for the necessity of the **Intent-Action Service (IAS)** separation in high-integrity research workflows. By grounding the system in deterministic data-states, ARGUS creates a structural safety framework that ensures scientific validity independently of AI model quality.

== Introduction: The Historical Context

The safety model of ARGUS was developed in response to the "Control-Flow Drift" observed in early legacy iterations of the project. Versions 1.0 through 9.0 relied on the Large Language Model (LLM) to maintain the authoritative state of the user session within its own context window. This approach consistently failed as research sessions became longer and more complex; the model's internal probability distribution would eventually shift away from the project's physical reality, leading to confident but incorrect procedural guidance.

We learned that safety in AI-mediated systems is not a property of "better prompting" or "RLHF alignment," but a property of **better architecture**. The resolution was the formalization of **Data-State Grounding**. By ensuring that every state transition is a verifiable mutation of a deterministic filesystem (the VFS), ARGUS creates a "structural safety" where the system's "Truth" is a physical property of the environment rather than a linguistic claim.

== The Irreducible Hallucination Problem: Probabilities vs. States

The core of the safety challenge lies in the fundamental difference between probabilistic token prediction and deterministic state transformation.

*   **The Multiplication of Error:** Let $P(s)$ be the probability that an AI correctly executes a single step in a workflow. For a sequence of $n$ autonomous steps, the total reliability is $P(s)^n$. As $n$ increases, the probability of a "Session Drift" approach 1.0, regardless of how high $P(s)$ is.
*   **The Truth Gap:** An AI can "believe" it has performed an action (e.g., "I have harmonized the data") because it generated the text describing the success. However, without a deterministic feedback loop from the environment, there is no mechanism to verify the claim.

ARGUS addresses this by enforcing an **asymmetric loop**: the AI proposes the *Intent*, but only the deterministic *Action* (the Plugin) can produce the *State* (the Artifact). The system's integrity is thus governed by the deterministic layer, not the probabilistic layer.

== The IAS Formalism: Intents, Actions, States

The ARGUS safety model is formally defined by the separation of the interaction into three discrete phases:

1.  **Intents ($\mathcal{I}$):** The high-entropy, noisy linguistic input from the human user.
2.  **Actions ($\mathcal{A}$):** The low-entropy, deterministic protocol commands dispatched to the Host.
3.  **States ($\mathcal{S}$):** The persistent, materialized artifacts in the project and session trees.

Safety is achieved by ensuring that $\mathcal{I} \rightarrow \mathcal{A}$ is the *only* role of the AI. The transformation $\mathcal{A} \rightarrow \mathcal{S}$ is handled by the `PluginHost` and `MerkleEngine`, which are entirely external to the AI's influence. This prevents "state-injection" attacks where an AI might attempt to claim a state transition without performing the corresponding action.

== Merkle Provenance as a Safety Proof

To ensure the integrity of the scientific process over time, ARGUS utilizes a Merkle Provenance Chain.

*   **Proof of Work:** Every artifact materialized by the `MerkleEngine` is a "Proof of Execution." The SHA-256 fingerprint anchors the artifact to its specific logic and its specific parentage in the workflow DAG.
*   **Drift Detection:** If any upstream state is modified, the downstream fingerprints become invalid ($[STALE]$). This cryptographic grounding provides a mathematical guarantee that no model can be trained on inconsistent or outdated data, creating a hard safety gate that is immune to AI hallucination.

== The Intent-Guard: Hardware-Enforced Honesty

To prevent the "Intent Theft" problem where a sophisticated model might attempt to map natural language to a valid but out-of-order workflow command, ARGUS implements the `IntentGuard` middleware. This module enforces safety through two primary mechanisms:

1.  **Vocabulary Jailing:** Before invoking the LLM, the `IntentGuard` filters the manifest command list to include only those stages currently marked as `READY` by the DAG Engine. By denying the model knowledge of future opcodes, we eliminate the primary surface for "Instructional Decay."
2.  **Output Validation:** Any intent returned by the probabilistic compiler is intercepted and verified against the ready set. If the model attempts to return an unauthorized command, the `IntentGuard` downgrades the intent to `CONVERSATIONAL`.

This "Jail and Intercept" pattern ensures that even in the presence of conversational drift, the system's deterministic core remains unbreachable. The guard is toggleable, allowing researchers to quantitatively measure the "Hallucination Delta" between strict and experimental modes.

== Conclusion

The ARGUS safety model provides a mathematically rigorous framework for embedding AI within high-stakes medical research environments. By grounding the conversational interface in a deterministic host virtual machine, we transform the AI from a decision-making authority into a high-fidelity mediator. This architecture ensures that scientific integrity is maintained through physical evidence and cryptographic proofs, rather than trusting the transient memory of a probabilistic model.

---
_Last updated: 2026-02-17 (v10.0.0)_
